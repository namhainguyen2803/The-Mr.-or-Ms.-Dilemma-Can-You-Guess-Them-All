{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.3.1-cp310-cp310-macosx_11_0_arm64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.7.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gensim) (1.8.1)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from gensim) (1.22.4)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.3.1 smart-open-6.3.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (22.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-23.1.2-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.3.1\n",
      "    Uninstalling pip-22.3.1:\n",
      "      Successfully uninstalled pip-22.3.1\n",
      "Successfully installed pip-23.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "# Create a list of sentences as input\n",
    "sentences = common_texts\n",
    "print(common_texts[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Get the word vector for a specific word\n",
    "word_vector = model.wv['computer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "# Find similar words to a given word\n",
    "similar_words = model.wv.most_similar('computer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Vector for 'computer': [-0.00515774 -0.00667028 -0.0077791   0.00831315 -0.00198292 -0.00685696\n",
      " -0.0041556   0.00514562 -0.00286997 -0.00375075  0.0016219  -0.0027771\n",
      " -0.00158482  0.0010748  -0.00297881  0.00852176  0.00391207 -0.00996176\n",
      "  0.00626142 -0.00675622  0.00076966  0.00440552 -0.00510486 -0.00211128\n",
      "  0.00809783 -0.00424503 -0.00763848  0.00926061 -0.00215612 -0.00472081\n",
      "  0.00857329  0.00428458  0.0043261   0.00928722 -0.00845554  0.00525685\n",
      "  0.00203994  0.0041895   0.00169839  0.00446543  0.00448759  0.0061063\n",
      " -0.00320303 -0.00457706 -0.00042664  0.00253447 -0.00326412  0.00605948\n",
      "  0.00415534  0.00776685  0.00257002  0.00811904 -0.00138761  0.00808028\n",
      "  0.0037181  -0.00804967 -0.00393476 -0.0024726   0.00489447 -0.00087241\n",
      " -0.00283173  0.00783599  0.00932561 -0.0016154  -0.00516075 -0.00470313\n",
      " -0.00484746 -0.00960562  0.00137242 -0.00422615  0.00252744  0.00561612\n",
      " -0.00406709 -0.00959937  0.00154715 -0.00670207  0.0024959  -0.00378173\n",
      "  0.00708048  0.00064041  0.00356198 -0.00273993 -0.00171105  0.00765502\n",
      "  0.00140809 -0.00585215 -0.00783678  0.00123304  0.00645651  0.00555797\n",
      " -0.00897966  0.00859466  0.00404815  0.00747178  0.00974917 -0.0072917\n",
      " -0.00904259  0.0058377   0.00939395  0.00350795]\n",
      "Similar Words to 'computer': [('system', 0.21617144346237183), ('survey', 0.044689204543828964), ('interface', 0.015203382819890976), ('time', 0.0019510675920173526), ('trees', -0.03284312039613724), ('human', -0.0742427259683609), ('response', -0.09317591041326523), ('graph', -0.09575346857309341), ('eps', -0.10513807088136673), ('user', -0.16911625862121582)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print the word vector and similar words\n",
    "print(\"Word Vector for 'computer':\", word_vector)\n",
    "print(\"Similar Words to 'computer':\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mmovie_reviews\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('movie_reviews')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/movie_reviews\u001b[0m\n\n  Searched in:\n    - '/Users/nguyenbathiem/nltk_data'\n    - '/Users/nguyenbathiem/opt/anaconda3/nltk_data'\n    - '/Users/nguyenbathiem/opt/anaconda3/share/nltk_data'\n    - '/Users/nguyenbathiem/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mzip_name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     85\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mmovie_reviews\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('movie_reviews')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/movie_reviews.zip/movie_reviews/\u001b[0m\n\n  Searched in:\n    - '/Users/nguyenbathiem/nltk_data'\n    - '/Users/nguyenbathiem/opt/anaconda3/nltk_data'\n    - '/Users/nguyenbathiem/opt/anaconda3/share/nltk_data'\n    - '/Users/nguyenbathiem/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcorpus\u001b[39;00m \u001b[39mimport\u001b[39;00m movie_reviews\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Load movie review corpus\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m sentences \u001b[39m=\u001b[39m movie_reviews\u001b[39m.\u001b[39;49msents()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Train the Word2Vec model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m Word2Vec(sentences\u001b[39m=\u001b[39msentences, min_count\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLazyCorpusLoader object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m__bases__\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__load()\n\u001b[1;32m    122\u001b[0m \u001b[39m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39m# __class__ to something new:\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, attr)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m             root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfind(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubdir\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mzip_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m:\n\u001b[0;32m---> 86\u001b[0m             \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m     88\u001b[0m \u001b[39m# Load the corpus.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m corpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__reader_cls(root, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m         root \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mfind(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msubdir\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__name\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     82\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mLookupError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     83\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[1;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mmovie_reviews\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('movie_reviews')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/movie_reviews\u001b[0m\n\n  Searched in:\n    - '/Users/nguyenbathiem/nltk_data'\n    - '/Users/nguyenbathiem/opt/anaconda3/nltk_data'\n    - '/Users/nguyenbathiem/opt/anaconda3/share/nltk_data'\n    - '/Users/nguyenbathiem/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "# Load movie review corpus\n",
    "sentences = movie_reviews.sents()\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences=sentences, min_count=1)\n",
    "\n",
    "# Get the word vector for a specific word\n",
    "word_vector = model.wv['movie']\n",
    "\n",
    "# Find similar words to a given word\n",
    "similar_words = model.wv.most_similar('movie')\n",
    "print(similar_words)\n",
    "# # Visualize word embeddings using t-SNE\n",
    "# def visualize_embeddings(model):\n",
    "#     vocab = list(model.wv.key_to_index.keys())\n",
    "#     vectors = model.wv[vocab]\n",
    "#     tsne = TSNE(n_components=2, random_state=42)\n",
    "#     embeddings = tsne.fit_transform(vectors)\n",
    "\n",
    "#     x = embeddings[:, 0]\n",
    "#     y = embeddings[:, 1]\n",
    "\n",
    "#     plt.figure(figsize=(10, 8))\n",
    "#     plt.scatter(x, y, marker='o')\n",
    "\n",
    "#     for i, word in enumerate(vocab):\n",
    "#         plt.annotate(word, (x[i], y[i]))\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "# # Visualize the word embeddings\n",
    "# visualize_embeddings(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'ellipsis' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb Cell 8\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m sentences \u001b[39m=\u001b[39m [\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     [\u001b[39m'\u001b[39m\u001b[39mthis\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mis\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mthe\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mfirst\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     [\u001b[39m'\u001b[39m\u001b[39mthis\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mis\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mthe\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msecond\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb#X10sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     [\u001b[39m'\u001b[39m\u001b[39myet\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39manother\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Train the Word2Vec model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m model \u001b[39m=\u001b[39m Word2Vec(sentences, min_count\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Get the vector representation of each sentence\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m sentence_vectors \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/word2vec.py:425\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39mif\u001b[39;00m corpus_iterable \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m corpus_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    424\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39m(epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[0;32m--> 425\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_vocab(corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file, trim_rule\u001b[39m=\u001b[39;49mtrim_rule)\n\u001b[1;32m    426\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain(\n\u001b[1;32m    427\u001b[0m         corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, total_examples\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_count,\n\u001b[1;32m    428\u001b[0m         total_words\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_total_words, epochs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs, start_alpha\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha,\n\u001b[1;32m    429\u001b[0m         end_alpha\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_alpha, compute_loss\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss, callbacks\u001b[39m=\u001b[39mcallbacks)\n\u001b[1;32m    430\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/word2vec.py:487\u001b[0m, in \u001b[0;36mWord2Vec.build_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[39m\"\"\"Build vocabulary from a sequence of sentences (can be a once-only generator stream).\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \n\u001b[1;32m    451\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    484\u001b[0m \n\u001b[1;32m    485\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 487\u001b[0m total_words, corpus_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_vocab(\n\u001b[1;32m    488\u001b[0m     corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file, progress_per\u001b[39m=\u001b[39;49mprogress_per, trim_rule\u001b[39m=\u001b[39;49mtrim_rule)\n\u001b[1;32m    489\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_count \u001b[39m=\u001b[39m corpus_count\n\u001b[1;32m    490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_total_words \u001b[39m=\u001b[39m total_words\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/word2vec.py:582\u001b[0m, in \u001b[0;36mWord2Vec.scan_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[39mif\u001b[39;00m corpus_file:\n\u001b[1;32m    580\u001b[0m     corpus_iterable \u001b[39m=\u001b[39m LineSentence(corpus_file)\n\u001b[0;32m--> 582\u001b[0m total_words, corpus_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_scan_vocab(corpus_iterable, progress_per, trim_rule)\n\u001b[1;32m    584\u001b[0m logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    585\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcollected \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m word types from a corpus of \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m raw words and \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m sentences\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    586\u001b[0m     \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_vocab), total_words, corpus_count\n\u001b[1;32m    587\u001b[0m )\n\u001b[1;32m    589\u001b[0m \u001b[39mreturn\u001b[39;00m total_words, corpus_count\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/gensim/models/word2vec.py:565\u001b[0m, in \u001b[0;36mWord2Vec._scan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[39mif\u001b[39;00m sentence_no \u001b[39m%\u001b[39m progress_per \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    561\u001b[0m     logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    562\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPROGRESS: at sentence #\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m, processed \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m words, keeping \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m word types\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    563\u001b[0m         sentence_no, total_words, \u001b[39mlen\u001b[39m(vocab),\n\u001b[1;32m    564\u001b[0m     )\n\u001b[0;32m--> 565\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m sentence:\n\u001b[1;32m    566\u001b[0m     vocab[word] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    567\u001b[0m total_words \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(sentence)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'ellipsis' object is not iterable"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Define your list of sentences\n",
    "sentences = [\n",
    "    ['this', 'is', 'the', 'first', 'sentence'],\n",
    "    ['this', 'is', 'the', 'second', 'sentence'],\n",
    "    ['yet', 'another', 'sentence'],\n",
    "    ...\n",
    "]\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "# Get the vector representation of each sentence\n",
    "sentence_vectors = []\n",
    "for sentence in sentences:\n",
    "    vector = sum(model.wv[word] for word in sentence) / len(sentence)  # Average word vectors in the sentence\n",
    "    sentence_vectors.append(vector)\n",
    "\n",
    "# Print the sentence vectors\n",
    "for i, vector in enumerate(sentence_vectors):\n",
    "    print(f\"Sentence {i+1}: {vector}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'text8.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb Cell 10\u001b[0m in \u001b[0;36m9\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m sentences \u001b[39m=\u001b[39m Text8Corpus(corpus_path)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb#X11sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Train the Word2Vec model\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb#X11sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m model \u001b[39m=\u001b[39m Word2Vec(sentences\u001b[39m=\u001b[39;49msentences, min_count\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb#X11sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Get the vector representation of each sentence\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/src/word2vec.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m sentence_vectors \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/models/word2vec.py:429\u001b[0m, in \u001b[0;36mWord2Vec.__init__\u001b[0;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[39mif\u001b[39;00m corpus_iterable \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m corpus_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39m(epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))\n\u001b[0;32m--> 429\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuild_vocab(corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file, trim_rule\u001b[39m=\u001b[39;49mtrim_rule)\n\u001b[1;32m    430\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain(\n\u001b[1;32m    431\u001b[0m         corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, total_examples\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_count,\n\u001b[1;32m    432\u001b[0m         total_words\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_total_words, epochs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs, start_alpha\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39malpha,\n\u001b[1;32m    433\u001b[0m         end_alpha\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_alpha, compute_loss\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss, callbacks\u001b[39m=\u001b[39mcallbacks)\n\u001b[1;32m    434\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/models/word2vec.py:491\u001b[0m, in \u001b[0;36mWord2Vec.build_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[39m\"\"\"Build vocabulary from a sequence of sentences (can be a once-only generator stream).\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \n\u001b[1;32m    455\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    488\u001b[0m \n\u001b[1;32m    489\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    490\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_corpus_sanity(corpus_iterable\u001b[39m=\u001b[39mcorpus_iterable, corpus_file\u001b[39m=\u001b[39mcorpus_file, passes\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 491\u001b[0m total_words, corpus_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscan_vocab(\n\u001b[1;32m    492\u001b[0m     corpus_iterable\u001b[39m=\u001b[39;49mcorpus_iterable, corpus_file\u001b[39m=\u001b[39;49mcorpus_file, progress_per\u001b[39m=\u001b[39;49mprogress_per, trim_rule\u001b[39m=\u001b[39;49mtrim_rule)\n\u001b[1;32m    493\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_count \u001b[39m=\u001b[39m corpus_count\n\u001b[1;32m    494\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcorpus_total_words \u001b[39m=\u001b[39m total_words\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/models/word2vec.py:586\u001b[0m, in \u001b[0;36mWord2Vec.scan_vocab\u001b[0;34m(self, corpus_iterable, corpus_file, progress_per, workers, trim_rule)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[39mif\u001b[39;00m corpus_file:\n\u001b[1;32m    584\u001b[0m     corpus_iterable \u001b[39m=\u001b[39m LineSentence(corpus_file)\n\u001b[0;32m--> 586\u001b[0m total_words, corpus_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_scan_vocab(corpus_iterable, progress_per, trim_rule)\n\u001b[1;32m    588\u001b[0m logger\u001b[39m.\u001b[39minfo(\n\u001b[1;32m    589\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcollected \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m word types from a corpus of \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m raw words and \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m sentences\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    590\u001b[0m     \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_vocab), total_words, corpus_count\n\u001b[1;32m    591\u001b[0m )\n\u001b[1;32m    593\u001b[0m \u001b[39mreturn\u001b[39;00m total_words, corpus_count\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/models/word2vec.py:555\u001b[0m, in \u001b[0;36mWord2Vec._scan_vocab\u001b[0;34m(self, sentences, progress_per, trim_rule)\u001b[0m\n\u001b[1;32m    553\u001b[0m vocab \u001b[39m=\u001b[39m defaultdict(\u001b[39mint\u001b[39m)\n\u001b[1;32m    554\u001b[0m checked_string_types \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 555\u001b[0m \u001b[39mfor\u001b[39;00m sentence_no, sentence \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sentences):\n\u001b[1;32m    556\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m checked_string_types:\n\u001b[1;32m    557\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(sentence, \u001b[39mstr\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gensim/models/word2vec.py:2052\u001b[0m, in \u001b[0;36mText8Corpus.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2048\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__iter__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   2049\u001b[0m     \u001b[39m# the entire corpus is one gigantic line -- there are no sentence marks at all\u001b[39;00m\n\u001b[1;32m   2050\u001b[0m     \u001b[39m# so just split the sequence of tokens arbitrarily: 1 sentence = 1000 tokens\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m     sentence, rest \u001b[39m=\u001b[39m [], \u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m-> 2052\u001b[0m     \u001b[39mwith\u001b[39;00m utils\u001b[39m.\u001b[39;49mopen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfname, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fin:\n\u001b[1;32m   2053\u001b[0m         \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m   2054\u001b[0m             text \u001b[39m=\u001b[39m rest \u001b[39m+\u001b[39m fin\u001b[39m.\u001b[39mread(\u001b[39m8192\u001b[39m)  \u001b[39m# avoid loading the entire file (=1 line) into RAM\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/smart_open/smart_open_lib.py:177\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39mif\u001b[39;00m transport_params \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    175\u001b[0m     transport_params \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 177\u001b[0m fobj \u001b[39m=\u001b[39m _shortcut_open(\n\u001b[1;32m    178\u001b[0m     uri,\n\u001b[1;32m    179\u001b[0m     mode,\n\u001b[1;32m    180\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[1;32m    181\u001b[0m     buffering\u001b[39m=\u001b[39;49mbuffering,\n\u001b[1;32m    182\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[1;32m    183\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    184\u001b[0m     newline\u001b[39m=\u001b[39;49mnewline,\n\u001b[1;32m    185\u001b[0m )\n\u001b[1;32m    186\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39mreturn\u001b[39;00m fobj\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/smart_open/smart_open_lib.py:363\u001b[0m, in \u001b[0;36m_shortcut_open\u001b[0;34m(uri, mode, compression, buffering, encoding, errors, newline)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[39mif\u001b[39;00m errors \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m    361\u001b[0m     open_kwargs[\u001b[39m'\u001b[39m\u001b[39merrors\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m errors\n\u001b[0;32m--> 363\u001b[0m \u001b[39mreturn\u001b[39;00m _builtin_open(local_path, mode, buffering\u001b[39m=\u001b[39;49mbuffering, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mopen_kwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'text8.txt'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "\n",
    "# Load Text8 corpus\n",
    "with open(\"/Users/nguyenbathiem/The-Mr.-or-Ms.-Dilemma-Can-You-Guess-Them-All/GenderPrediction/dataset/name_full.csv\") as file:\n",
    "    \n",
    "corpus_path = 'text8.txt'  # Path to the Text8 corpus file\n",
    "sentences = Text8Corpus(corpus_path)\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences=sentences, min_count=1)\n",
    "\n",
    "# Get the vector representation of each sentence\n",
    "sentence_vectors = []\n",
    "for sentence in sentences:\n",
    "    vector = sum(model.wv[word] for word in sentence) / len(sentence)  # Average word vectors in the sentence\n",
    "    sentence_vectors.append(vector)\n",
    "\n",
    "# Print the sentence vectors\n",
    "for i, vector in enumerate(sentence_vectors):\n",
    "    print(f\"Sentence {i+1}: {vector}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
