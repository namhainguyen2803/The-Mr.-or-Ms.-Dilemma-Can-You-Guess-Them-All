{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the source of kmeans model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from TF_IDF import * \n",
    "import pandas as pd\n",
    "from helper_functions import train_test_split, calculate_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Full_Name</th>\n",
       "      <th>Gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ngô Xuân Tùng</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bùi Dương Thảo Vy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lưu Thế Huy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nguyễn Thị Vân</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dương Minh Long</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Full_Name  Gender\n",
       "0      Ngô Xuân Tùng       1\n",
       "1  Bùi Dương Thảo Vy       0\n",
       "2        Lưu Thế Huy       1\n",
       "3     Nguyễn Thị Vân       0\n",
       "4    Dương Minh Long       1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"name_full.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data=df['Full_Name']\n",
    "labels = df['Gender']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26851\n"
     ]
    }
   ],
   "source": [
    "list_of_names=full_data.tolist()\n",
    "list_of_labels = labels.to_numpy()\n",
    "print(len(full_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF_IDF_full_data = TF_IDF(list_of_names)\n",
    "onehot_data = np.array(TF_IDF_full_data.matrix_word_count)\n",
    "#svd = TruncatedSVD(n_components=500)\n",
    "#normalizer = Normalizer(copy=False)\n",
    "#X_normalized = normalizer.fit_transform(tfidf_data)\n",
    "#X_svd = svd.fit_transform(X_normalized)\n",
    "onehot_data[onehot_data != 0] = 1\n",
    "onehot_data = np.append(onehot_data, list_of_labels.reshape(-1,1), axis=1)\n",
    "train_data = onehot_data[:int(0.8*onehot_data.shape[1]),:]\n",
    "test_data = onehot_data[int(0.8*onehot_data.shape[1]):,:]\n",
    "#onehot_data_df = pd.DataFrame(onehot_data, columns = [str(i) for i in range(onehot_data.shape[1])])\n",
    "#print(onehot_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the data of each member \n",
    "class Member:\n",
    "    def __init__(self, r_d, label = None):\n",
    "        self._r_d = r_d\n",
    "        self._label = label\n",
    "#class Cluster\n",
    "class Cluster:\n",
    "    def __init__(self):\n",
    "        self._centroid = None\n",
    "        self._members=[]\n",
    "    #set centroid\n",
    "    def set_centroid(self,new_centroid):\n",
    "        self._centroid = new_centroid\n",
    "    #reset list of members\n",
    "    def reset_members(self):\n",
    "        self._members = []\n",
    "    #add new member\n",
    "    def add_member(self,member):\n",
    "        self._members.append(member)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kmeans:\n",
    "    \n",
    "    def __init__(self,num_clusters):\n",
    "        self._num_clusters = num_clusters\n",
    "        self._clusters = [Cluster() for _ in range(self._num_clusters)] #list of clusters\n",
    "        self._E = [] #list of centroids\n",
    "        self._S = 0 #overall similarity\n",
    "    \n",
    "    #load data\n",
    "    def load_data(self, data):\n",
    "        self._data = []\n",
    "        #store number of files in newsgroup\n",
    "        self._label_count = defaultdict(int)\n",
    "        #append data point to self._data\n",
    "        for d in data:\n",
    "            label = d[-1]\n",
    "            self._label_count[label]+=1\n",
    "            r_d = d[:-1]\n",
    "            #append member with class Member\n",
    "            self._data.append(Member(r_d=r_d,label=label))\n",
    "\n",
    "    #initialize centroid in a random way\n",
    "    def random_init(self,seed_value):\n",
    "        random.seed(seed_value)\n",
    "        #get the list of data points\n",
    "        members = [member._r_d for member in self._data]\n",
    "        #return a random array having length num_clusters, elements are less than len(self._data)\n",
    "        pos = np.random.choice(len(self._data), self._num_clusters, replace=False)\n",
    "        centroids=[]\n",
    "        for i in pos:\n",
    "            centroids.append(members[i])\n",
    "        self._E = centroids\n",
    "        # update centroid\n",
    "        for i in range(self._num_clusters):\n",
    "            self._clusters[i].set_centroid(centroids[i])\n",
    "    # using cosine similarity: S(x,y) = x.y / |x|*|y|       \n",
    "    def compute_similarity(self, member, centroid):\n",
    "        if type(member) == Member:\n",
    "            return centroid.dot(member._r_d) * 1. / np.linalg.norm(centroid)*np.linalg.norm(member._r_d)\n",
    "        else:\n",
    "            return centroid.dot(member) * 1. / np.linalg.norm(centroid)*np.linalg.norm(member)\n",
    "    #select cluster for a data point\n",
    "    def select_cluster_for(self,member):\n",
    "        best_fit_cluster = None\n",
    "        #initialize max_similarity: cos(x)=-1 => x=180\n",
    "        max_similarity = -1\n",
    "        #search in set of clusters\n",
    "        for cluster in self._clusters:\n",
    "            #calculate the similarity between data point and centroid\n",
    "            similarity = self.compute_similarity(member,cluster._centroid)\n",
    "            if similarity > max_similarity:\n",
    "                best_fit_cluster = cluster\n",
    "                max_similarity = similarity\n",
    "        #add data point to cluster\n",
    "        best_fit_cluster.add_member(member)\n",
    "        return max_similarity\n",
    "    #update centroid of a cluster\n",
    "    def update_centroid_of(self,cluster):\n",
    "        #crawl list of tfidf vector\n",
    "        member_r_ds = [member._r_d for member in cluster._members]\n",
    "        #calculate the new centroid\n",
    "        aver_r_d = np.mean(member_r_ds, axis=0)\n",
    "        sqrt_sum_sqr = np.sqrt(np.sum(aver_r_d ** 2))\n",
    "        new_centroid = np.array([value/sqrt_sum_sqr for value in aver_r_d])\n",
    "        #update new centroid\n",
    "        cluster._centroid = new_centroid\n",
    "    #check for stopping condition\n",
    "    def stopping_condition(self, criterion, threshold):\n",
    "        #list of criteria\n",
    "        criteria = ['max_iters','similarity','centroid']\n",
    "        assert criterion in criteria\n",
    "        #check conditions of each criterion\n",
    "        if criterion == 'max_iters':\n",
    "            if self._iterations >= threshold:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        elif criterion == 'centroid':\n",
    "            E_new = [list(cluster._centroid) for cluster in self._clusters]\n",
    "            E_new_minus_E = [centroid for centroid in E_new if centroid not in self._E]\n",
    "            self._E = E_new\n",
    "            if len(E_new_minus_E) < threshold:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            new_S_minus_S = self._new_S - self._S\n",
    "            self._S = self._new_S\n",
    "            if new_S_minus_S < threshold:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    def run(self, seed_value, criterion, threshold):\n",
    "        #randomly initialize set of centroids and number of iteration\n",
    "        self.random_init(seed_value)\n",
    "        self._iteration = 0\n",
    "        #run process\n",
    "        while True:\n",
    "            #reset members of each cluster\n",
    "            for cluster in self._clusters:\n",
    "                cluster.reset_members()\n",
    "            self._new_S = 0\n",
    "            #choose cluster for each member\n",
    "            for member in self._data:\n",
    "                max_s = self.select_cluster_for(member)\n",
    "                self._new_S += max_s\n",
    "            #determine new centroid of each cluster\n",
    "            for cluster in self._clusters:\n",
    "                self.update_centroid_of(cluster)\n",
    "            \n",
    "            self._iteration += 1\n",
    "            #check for stopping condition\n",
    "            if self.stopping_condition(criterion, threshold):\n",
    "                break\n",
    "    #compute purity value\n",
    "    def compute_purity(self):\n",
    "        majority_sum = 0\n",
    "        for cluster in self._clusters:\n",
    "            #set of label of members of cluster\n",
    "            member_labels = [member._label for member in cluster._members]\n",
    "            #maximum of numbers of data points of label, note: we have 20 labels\n",
    "            max_count = max([member_labels.count(label) for label in range(2)])\n",
    "            majority_sum+=max_count\n",
    "        return majority_sum * 1. / len(self._data)\n",
    "    #compute NMI value\n",
    "    def compute_NMI(self):\n",
    "        I_value, H_omega, H_C, N = 0. ,0. , 0. ,len(self._data)\n",
    "        for cluster in self._clusters:\n",
    "            wk = len(cluster._members) * 1.\n",
    "            H_omega += -wk/N * np.log10(wk/N)\n",
    "            member_labels = [member._label for member in cluster._members]\n",
    "            for label in range(1):\n",
    "                wk_cj = member_labels.count(label) * 1.\n",
    "                cj = self._label_count[label]\n",
    "                I_value += wk_cj / N * np.log10(N * wk_cj/(wk*cj) + 1e-12)\n",
    "        for label in range(20):\n",
    "            cj = self._label_count[label]\n",
    "            H_C += -cj/N * np.log10(cj/N)\n",
    "        return I_value * 2. / (H_omega + H_C)\n",
    "    def label_of_cluster(self, cluster):\n",
    "        member_labels = [member._label for member in cluster._members]\n",
    "        member_labels = np.array(member_labels)\n",
    "        values, counts = np.unique(member_labels, return_counts=True)\n",
    "        return values[counts.argmax()]\n",
    "\n",
    "    def prediction(self, examples, labels):\n",
    "        predictions=[]\n",
    "        for example in examples:\n",
    "            max_distance = -1\n",
    "            for cluster in self._clusters:\n",
    "                distance = self.compute_similarity(example, cluster._centroid)\n",
    "                if distance>max_distance:\n",
    "                    max_distance = distance\n",
    "                    flag=self.label_of_cluster(cluster)\n",
    "            predictions.append(flag)\n",
    "        return calculate_accuracy(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purity:0.8315789473684211\n",
      "0.8335805746408494\n"
     ]
    }
   ],
   "source": [
    "kmeans = Kmeans(num_clusters=8)\n",
    "kmeans.load_data(train_data)\n",
    "kmeans.run(seed_value=42, criterion='similarity', threshold=1e-3)\n",
    "print(\"Purity:{}\".format(kmeans.compute_purity()))\n",
    "print(kmeans.prediction(examples=test_data[:,:-1], labels=test_data[:,-1]))\n",
    "#print(test_data[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
